# 海量数据处理算法总结  
## topK问题   
快速找出海量数据的topK, 最简单的方法为利用堆heap实现,比如找出top10大的数据，建立一个大小为10的最小堆, 如果当前元素大于堆顶元素则排除堆顶元素加入新元素。  

``` c++
#include <iostream>                                                             
#include <algorithm>                                                            
#define K 10                                                                    
                                                                                
using namespace std;                                                            
                                                                                
int heap[K+1];                                                                  
                                                                                
void adjustHeap(int pos){                                                       
    int left = pos*2;                                                           
    int right = pos*2+1;                                                        
    int child;                                                                  
    if(left > K)                                                                
        return;                                                                 
    if(right > K)                                                               
        child = left;                                                           
    else                                                                        
        child = heap[left] < heap[right] ? left :  right;                       
    if (heap[child] < heap[pos]){                                               
        int tmp = heap[pos];                                                    
        heap[pos] = heap[child];                                                
        heap[child] = tmp;                                                      
        adjustHeap(child);   // adjust the child                                
    }                                                                           
                                                                                
}                                                                               
                                                                                
void creatHeap(){                                                               
    for(int pos = K/2 ; pos > 1;  pos--){                                       
        adjustHeap(pos);                                                        
    }                                                                           
                                                                                
}                                                                               
int main()                                                                      
{                                                                               
    int array[] = {34,55,12,23,78,20,22,90,46,35,3,89,10,25,98,100,234,56,87,333,789,452,761};     
    int n = sizeof(array)/sizeof(int);                                          
    cout << "array size = "<< n <<endl;                                         
    for(int i =0; i < K; i++){                                                  
        heap[i+1] = array[i];                                                   
    }                                                                           
                                                                                
    creatHeap();                                                                
                                       
    for(int i = K+1; i < n  ; i++ ){ 
         if(array[i] > heap[1]){      
             heap[1] = array[i];      
             adjustHeap(1);           
         }                            
                                      
    }                                
    for(int i=0; i<K; i++)            
        cout << heap[i] << " ";       
                                      
    cout <<endl;                      
 }                                                                                                            

```

## 海量数据等概率随机抽样
How could you select one of n objects at random, where you see the objects sequentially but you do not know the value of n beforehand? For concreteness, how would you read a text file, and select and print one random line, when you don’t know the number of lines in advance?

问题定义可以简化如下：在不知道文件总行数的情况下，如何从文件中随机的抽取一行？

首先想到的是我们做过类似的题目吗?当然，在知道文件行数的情况下，我们可以很容易的用C运行库的rand函数随机的获得一个行数，从而随机的取出一行，但是，当前的情况是不知道行数，这样如何求呢？我们需要一个概念来帮助我们做出猜想，来使得对每一行取出的概率相等，也即随机。这个概念即蓄水池抽样（Reservoir Sampling）。

有了这个概念，我们便有了这样一个解决方案：定义取出的行号为choice，第一次直接以第一行作为取出行 choice ，而后第二次以二分之一概率决定是否用第二行替换 choice ，第三次以三分之一的概率决定是否以第三行替换 choice ……，以此类推，可用伪代码描述如下：

``` python
i = 0
while more input lines
     with probability 1.0/++i
             choice = this input line
print choice
```

这种方法的巧妙之处在于成功的构造出了一种方式使得最后可以证明对每一行的取出概率都为1/n（其中n为当前扫描到的文件行数），换句话说对每一行取出的概率均相等，也即完成了随机的选取。

回顾这个问题，我们可以对其进行扩展，即如何从未知或者很大样本空间随机地取k个数？

类比下即可得到答案，即先把前k个数放入蓄水池，对第k+1，我们以k/(k+1)概率决定是否要把它换入蓄水池，换入时随机的选取一个作为替换项，这样一直做下去，对于任意的样本空间n，对每个数的选取概率都为k/n。也就是说对每个数选取概率相等。

``` python
Init : a reservoir with the size： k
for i= k+1 to N
    M=random(1, i);
    if( M < k)
     SWAP the Mth value and ith value
end for 
```

## 海量日志数据，提取出某日访问百度次数最多的那个IP

分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决

hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。

堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。


IP地址最多有2^32=4G种取值可能，所以不能完全加载到内存中。
可以考虑分而治之的策略，按照IP地址的hash(IP)%1024值，将海量日志存储到1024个小文件中。每个小文件最多包含4M个IP地址。
对于每个小文件，可以构建一个IP作为key，出现次数作为value的hash_map，并记录当前出现次数最多的1个IP地址。
有了1024个小文件中的出现次数最多的IP，我们就可以轻松得到总体上出现次数最多的IP。

## 一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，返回频数最高的100个词

分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。
堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。

## 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为，这里漏写个了a1）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为）。这样处理后，所有可能相同的url都在对应的小文件（）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
   

 ## 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。
 
 有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。
 
## 5亿个int找它们的中位数

思路一：这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。
	
 实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。
 

思路二：同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。
 
方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。

第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该<n/2（2.5亿）。而k+1 - 65535的计数和小于n/2，第二遍统计同上面的方法类似，但这次只统计处于区间k的情况，也就是说(x / 65536) + 32768 = k。统计只统计低16位的情况。并且利用刚才统计的sum，比如sum = 2.49亿，那么现在就是要在低16位里面找100万个数(2.5亿-2.49亿)。这次计数之后，再统计一下，看中位数所处的区间，最后将高位和低位组合一下就是结果了。

